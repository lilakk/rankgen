t5_encoder is in training mode: False
using loss function cosine_similarity with False penalty, alpha = 0.1
init suffix token: sharp
EPOCH 0
  cosine_similarity_loss: 0.6829383969306946
  suffix vector: tensor([ 0.0826,  0.1125, -0.1757,  ..., -0.2911,  0.2079,  0.0310],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 8.280710220336914
  l2 dist from vocab: 631.3866577148438
  learned embed: Parameter containing:
tensor([[-1.5581,  0.7511, -5.7672,  ..., 27.4341, -8.6627,  2.8990]],
       device='cuda:0', requires_grad=True)
EPOCH 500
  cosine_similarity_loss: 0.2610325813293457
  suffix vector: tensor([ 0.0181,  0.1537,  0.0193,  ..., -0.0565,  0.0953,  0.1254],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 4.305947303771973
  l2 dist from vocab: 701.5776977539062
  learned embed: Parameter containing:
tensor([[12.1986, -6.7499,  9.1509,  ..., 12.4327,  3.7990, -9.9123]],
       device='cuda:0', requires_grad=True)
EPOCH 1000
  cosine_similarity_loss: 0.22157061100006104
  suffix vector: tensor([ 0.0582,  0.1912, -0.0256,  ..., -0.0419,  0.0853,  0.1365],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 3.975795030593872
  l2 dist from vocab: 703.432861328125
  learned embed: Parameter containing:
tensor([[11.9415, -6.5005,  8.1730,  ..., 11.0246,  4.6852, -9.7606]],
       device='cuda:0', requires_grad=True)
EPOCH 1500
  cosine_similarity_loss: 0.18459004163742065
  suffix vector: tensor([ 0.0336,  0.2153,  0.0118,  ..., -0.0123,  0.0937,  0.0851],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 3.654892921447754
  l2 dist from vocab: 704.5904541015625
  learned embed: Parameter containing:
tensor([[ 11.7025,  -6.3619,   8.0082,  ...,  10.3597,   4.7320, -10.0133]],
       device='cuda:0', requires_grad=True)
token after optim: sharp
suffix seq:  sharp
using loss function cosine_similarity with False penalty, alpha = 0.1
init suffix token: K
EPOCH 0
  cosine_similarity_loss: 0.6923666000366211
  suffix vector: tensor([ 0.0771,  0.1038, -0.1861,  ..., -0.2897,  0.2095,  0.0435],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 8.39802360534668
  l2 dist from vocab: 552.665283203125
  learned embed: Parameter containing:
tensor([[ 12.1723,  -0.7647,   3.5351,  ...,   9.2470,   1.6784, -13.3938]],
       device='cuda:0', requires_grad=True)
EPOCH 500
  cosine_similarity_loss: 0.2586716413497925
  suffix vector: tensor([-0.0228,  0.1164,  0.0300,  ..., -0.1380,  0.0422,  0.1557],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 4.258565902709961
  l2 dist from vocab: 605.6857299804688
  learned embed: Parameter containing:
tensor([[  9.8739,   0.1058,   4.6103,  ...,   9.7787,   8.3518, -17.3092]],
       device='cuda:0', requires_grad=True)
EPOCH 1000
  cosine_similarity_loss: 0.21304291486740112
  suffix vector: tensor([-0.0261,  0.1312, -0.0053,  ..., -0.1013,  0.0654,  0.1316],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 3.893690347671509
  l2 dist from vocab: 608.4114990234375
  learned embed: Parameter containing:
tensor([[  8.7886,   0.4217,   4.1462,  ...,   9.8179,  10.3022, -15.8478]],
       device='cuda:0', requires_grad=True)
EPOCH 1500
  cosine_similarity_loss: 0.3443058729171753
  suffix vector: tensor([ 0.0126,  0.1144,  0.0448,  ..., -0.1584,  0.1193,  0.0965],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 4.956369400024414
  l2 dist from vocab: 610.3113403320312
  learned embed: Parameter containing:
tensor([[  8.2742,   0.3351,   3.6655,  ...,   9.8244,  11.2468, -15.6441]],
       device='cuda:0', requires_grad=True)
token after optim: K
suffix seq:  sharp K
using loss function cosine_similarity with False penalty, alpha = 0.1
init suffix token: Legion
EPOCH 0
  cosine_similarity_loss: 0.6948572993278503
  suffix vector: tensor([ 0.0921,  0.1140, -0.1801,  ..., -0.2738,  0.2191,  0.0409],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 8.363788604736328
  l2 dist from vocab: 767.2876586914062
  learned embed: Parameter containing:
tensor([[  6.5744, -18.5911,  -3.8720,  ...,   0.1284,  12.5735,   5.7896]],
       device='cuda:0', requires_grad=True)
EPOCH 500
  cosine_similarity_loss: 0.19372916221618652
  suffix vector: tensor([ 0.0201,  0.1434, -0.0741,  ..., -0.0830,  0.1242,  0.1430],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 3.7619829177856445
  l2 dist from vocab: 822.761962890625
  learned embed: Parameter containing:
tensor([[  4.9094, -13.4369,  -3.9228,  ...,  -5.9492,   5.0466,  -4.7919]],
       device='cuda:0', requires_grad=True)
EPOCH 1000
  cosine_similarity_loss: 0.15729713439941406
  suffix vector: tensor([ 0.0295,  0.1767, -0.0414,  ..., -0.0868,  0.1013,  0.1534],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 3.4013473987579346
  l2 dist from vocab: 825.3421630859375
  learned embed: Parameter containing:
tensor([[  4.6029, -11.2720,  -3.9753,  ...,   1.4359,   4.9536,  -4.6757]],
       device='cuda:0', requires_grad=True)
EPOCH 1500
  cosine_similarity_loss: 0.14522695541381836
  suffix vector: tensor([ 0.0157,  0.1634, -0.0137,  ..., -0.0691,  0.0748,  0.1567],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 3.2748684883117676
  l2 dist from vocab: 827.4479370117188
  learned embed: Parameter containing:
tensor([[ 4.2813, -9.5063, -3.9488,  ...,  4.7946,  4.5294, -4.8249]],
       device='cuda:0', requires_grad=True)
token after optim: Legion
suffix seq:  sharp K Legion
