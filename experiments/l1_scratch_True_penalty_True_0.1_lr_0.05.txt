t5_encoder is in training mode: False
using loss function l1 with True penalty, alpha = 0.1
init suffix token: monument
EPOCH 0
  l1_loss: 2587.625
  suffix vector: tensor([-0.2418, -0.0952, -0.1044,  ...,  0.0728, -0.1037, -0.0331],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 8.084630012512207
  vector l1 distance: 295.1926574707031
  l2 dist from vocab: 643.8623046875
  l1 dist from vocab: 22924.322265625
  learned embed: Parameter containing:
tensor([[27.0461, -7.2848, 10.7645,  ...,  0.7569, -2.5571, -4.0828]],
       device='cuda:0', requires_grad=True)
STOPPING AT EPOCH 363
token after optim: mes
suffix seq:  mes
using loss function l1 with True penalty, alpha = 0.1
init suffix token: Whole
EPOCH 0
  l1_loss: 2742.42138671875
  suffix vector: tensor([ 0.0571, -0.0564,  0.0503,  ..., -0.0675,  0.1198,  0.1139],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 8.290716171264648
  vector l1 distance: 298.2518310546875
  l2 dist from vocab: 686.6615600585938
  l1 dist from vocab: 24441.697265625
  learned embed: Parameter containing:
tensor([[ 2.2843,  2.2373, -8.0198,  ...,  0.1233, 15.6030, -4.2640]],
       device='cuda:0', requires_grad=True)
STOPPING AT EPOCH 309
token after optim: mes
suffix seq:  mes mes
using loss function l1 with True penalty, alpha = 0.1
init suffix token: guidelines
EPOCH 0
  l1_loss: 2505.3388671875
  suffix vector: tensor([-0.0824,  0.1263,  0.1118,  ..., -0.2016,  0.2235, -0.1545],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 8.657669067382812
  vector l1 distance: 313.8439025878906
  l2 dist from vocab: 615.2794799804688
  l1 dist from vocab: 21914.947265625
  learned embed: Parameter containing:
tensor([[-6.3940, -7.0088, 13.8710,  ..., 29.0843, -9.7546, 11.5979]],
       device='cuda:0', requires_grad=True)
STOPPING AT EPOCH 346
token after optim: mes
suffix seq:  mes mes mes
