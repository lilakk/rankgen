t5_encoder is in training mode: False
using loss function l2 with True penalty, alpha = 0.05
init suffix token: Strecke
EPOCH 0
  l2_loss: 46.980613708496094
  suffix vector: tensor([-0.1121,  0.0699, -0.0830,  ...,  0.0452,  0.0216,  0.0510],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 8.416131973266602
  l2 dist from vocab: 771.28955078125
  learned embed: Parameter containing:
tensor([[ -0.1871,   1.3768,  -7.7153,  ..., -18.9814, -28.6163, -18.1377]],
       device='cuda:0', requires_grad=True)
EPOCH 500
  l2_loss: 40.43113708496094
  suffix vector: tensor([-0.0758, -0.0625, -0.0547,  ...,  0.0955,  0.0764,  0.1237],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 7.308796405792236
  l2 dist from vocab: 662.44677734375
  learned embed: Parameter containing:
tensor([[  4.7255,  -2.4936,  -3.5182,  ..., -14.0341, -23.4704, -13.2724]],
       device='cuda:0', requires_grad=True)
EPOCH 1000
  l2_loss: 35.523433685302734
  suffix vector: tensor([-0.1561, -0.0467, -0.0832,  ..., -0.0038,  0.2487,  0.1210],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 6.214081287384033
  l2 dist from vocab: 586.18701171875
  learned embed: Parameter containing:
tensor([[  6.8444,  -1.1785,  -0.7251,  ...,  -9.8264, -18.9537,  -9.1198]],
       device='cuda:0', requires_grad=True)
EPOCH 1500
  l2_loss: 32.41402053833008
  suffix vector: tensor([-0.0801, -0.0286, -0.1072,  ..., -0.0580,  0.2250,  0.0066],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 5.311186790466309
  l2 dist from vocab: 542.0567016601562
  learned embed: Parameter containing:
tensor([[  8.1873,   1.4375,   3.1243,  ...,  -6.7054, -14.6481,  -5.0480]],
       device='cuda:0', requires_grad=True)
token after optim: mes
suffix seq:  mes
using loss function l2 with True penalty, alpha = 0.05
init suffix token: meu
EPOCH 0
  l2_loss: 41.89624786376953
  suffix vector: tensor([-0.1595,  0.0131,  0.1526,  ...,  0.0265,  0.0512, -0.0405],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 8.2570161819458
  l2 dist from vocab: 672.78466796875
  learned embed: Parameter containing:
tensor([[  7.6338, -13.7959,   7.1454,  ...,  13.6790,   6.5424,  -4.4338]],
       device='cuda:0', requires_grad=True)
EPOCH 500
  l2_loss: 35.97040557861328
  suffix vector: tensor([-0.1798, -0.0505,  0.0743,  ..., -0.0226,  0.0527, -0.0230],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 6.846527576446533
  l2 dist from vocab: 582.4775390625
  learned embed: Parameter containing:
tensor([[ 3.6602, -9.0705,  2.6673,  ...,  9.1414,  1.7917, -5.3866]],
       device='cuda:0', requires_grad=True)
EPOCH 1000
  l2_loss: 32.43951416015625
  suffix vector: tensor([-0.1451, -0.0241, -0.0348,  ..., -0.0022,  0.1475, -0.0062],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 5.761619567871094
  l2 dist from vocab: 533.5579223632812
  learned embed: Parameter containing:
tensor([[ 3.8660, -4.7543,  1.1143,  ...,  4.8241,  0.6646, -2.0964]],
       device='cuda:0', requires_grad=True)
EPOCH 1500
  l2_loss: 30.19917106628418
  suffix vector: tensor([-0.0908,  0.0263, -0.0729,  ..., -0.0760,  0.1633,  0.0785],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 4.716888904571533
  l2 dist from vocab: 509.6456604003906
  learned embed: Parameter containing:
tensor([[ 1.9998, -0.3642,  3.1386,  ...,  0.7842,  0.6499, -1.0261]],
       device='cuda:0', requires_grad=True)
token after optim: mes
suffix seq:  mes mes
using loss function l2 with True penalty, alpha = 0.05
init suffix token: Tickets
EPOCH 0
  l2_loss: 41.30464172363281
  suffix vector: tensor([ 0.1120, -0.0235,  0.0500,  ...,  0.0508, -0.0941, -0.1833],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 8.165924072265625
  l2 dist from vocab: 662.7743530273438
  learned embed: Parameter containing:
tensor([[ -1.5608,  -1.7526,   5.4315,  ..., -24.1244,  -6.0552, -10.7937]],
       device='cuda:0', requires_grad=True)
EPOCH 500
  l2_loss: 35.31072998046875
  suffix vector: tensor([-0.0895, -0.0584,  0.0613,  ...,  0.0097,  0.0545, -0.1013],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 6.7034454345703125
  l2 dist from vocab: 572.1456298828125
  learned embed: Parameter containing:
tensor([[  1.7675,  -0.9116,   0.4788,  ..., -18.9276,  -3.7667,  -6.5697]],
       device='cuda:0', requires_grad=True)
EPOCH 1000
  l2_loss: 31.916501998901367
  suffix vector: tensor([-0.1045, -0.0734, -0.0613,  ..., -0.0722,  0.1969, -0.0136],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 5.619694232940674
  l2 dist from vocab: 525.9361572265625
  learned embed: Parameter containing:
tensor([[  1.4432,   4.4213,   0.3503,  ..., -14.4283,   3.0512,  -2.5988]],
       device='cuda:0', requires_grad=True)
EPOCH 1500
  l2_loss: 29.379819869995117
  suffix vector: tensor([-0.0546, -0.0189, -0.0058,  ..., -0.1110,  0.1062,  0.0283],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 4.139551639556885
  l2 dist from vocab: 504.80535888671875
  learned embed: Parameter containing:
tensor([[ -0.4044,   3.7661,  -0.3291,  ..., -10.7646,   2.3618,  -0.9263]],
       device='cuda:0', requires_grad=True)
token after optim: Its
suffix seq:  mes mes Its
