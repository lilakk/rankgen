t5_encoder is in training mode: False
using loss function cosine_similarity with True penalty, alpha = 0.1
init suffix token: local
EPOCH 0
  cosine_similarity_loss: 56.18703842163086
  suffix vector: tensor([ 0.0792,  0.1146, -0.1745,  ..., -0.2769,  0.2144,  0.0272],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 8.345338821411133
  l2 dist from vocab: 554.9960327148438
  learned embed: Parameter containing:
tensor([[-4.7612, -1.0804, -1.9148,  ..., 15.1030, -1.3969, -5.1998]],
       device='cuda:0', requires_grad=True)
EPOCH 500
  cosine_similarity_loss: 47.825130462646484
  suffix vector: tensor([ 0.0695,  0.1054, -0.2029,  ..., -0.2373,  0.1846,  0.0629],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 7.910367012023926
  l2 dist from vocab: 471.68157958984375
  learned embed: Parameter containing:
tensor([[ 0.2200, -1.6630, -0.1620,  ...,  0.6420, -0.1873, -0.5155]],
       device='cuda:0', requires_grad=True)
EPOCH 1000
  cosine_similarity_loss: 47.808143615722656
  suffix vector: tensor([ 0.0710,  0.1089, -0.2041,  ..., -0.2296,  0.1794,  0.0725],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 7.84177827835083
  l2 dist from vocab: 471.57196044921875
  learned embed: Parameter containing:
tensor([[ 0.1835, -1.5388, -0.0305,  ..., -0.1365, -0.2283, -0.5535]],
       device='cuda:0', requires_grad=True)
EPOCH 1500
  cosine_similarity_loss: 47.808135986328125
  suffix vector: tensor([ 0.0710,  0.1089, -0.2041,  ..., -0.2295,  0.1793,  0.0725],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 7.841787338256836
  l2 dist from vocab: 471.57183837890625
  learned embed: Parameter containing:
tensor([[ 0.1832, -1.5368, -0.0301,  ..., -0.1378, -0.2289, -0.5533]],
       device='cuda:0', requires_grad=True)
token after optim: serrure
suffix seq:  serrure
using loss function cosine_similarity with True penalty, alpha = 0.1
init suffix token: deployed
EPOCH 0
  cosine_similarity_loss: 61.24556350708008
  suffix vector: tensor([ 0.0710,  0.0846, -0.1900,  ..., -0.2738,  0.1906,  0.0401],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 8.250455856323242
  l2 dist from vocab: 605.6406860351562
  learned embed: Parameter containing:
tensor([[ 3.5696, -9.8722,  1.2373,  ..., -2.9751,  0.2903, -5.7396]],
       device='cuda:0', requires_grad=True)
EPOCH 500
  cosine_similarity_loss: 47.931541442871094
  suffix vector: tensor([ 0.0641,  0.1024, -0.1991,  ..., -0.2504,  0.1979,  0.0547],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 8.013862609863281
  l2 dist from vocab: 472.6337585449219
  learned embed: Parameter containing:
tensor([[ 0.1403, -1.6984, -0.5994,  ..., -0.1172, -0.1585, -0.5136]],
       device='cuda:0', requires_grad=True)
EPOCH 1000
  cosine_similarity_loss: 47.80934143066406
  suffix vector: tensor([ 0.0729,  0.1077, -0.2040,  ..., -0.2291,  0.1772,  0.0709],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 7.843727111816406
  l2 dist from vocab: 471.58123779296875
  learned embed: Parameter containing:
tensor([[ 0.2081, -1.6101, -0.0688,  ..., -0.1110, -0.2098, -0.5568]],
       device='cuda:0', requires_grad=True)
EPOCH 1500
  cosine_similarity_loss: 47.808048248291016
  suffix vector: tensor([ 0.0733,  0.1085, -0.2041,  ..., -0.2285,  0.1772,  0.0709],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 7.8385162353515625
  l2 dist from vocab: 471.5743408203125
  learned embed: Parameter containing:
tensor([[ 0.2137, -1.5921, -0.0494,  ..., -0.1122, -0.2096, -0.5628]],
       device='cuda:0', requires_grad=True)
token after optim: serrure
suffix seq:  serrure serrure
using loss function cosine_similarity with True penalty, alpha = 0.1
init suffix token: disponibil
EPOCH 0
  cosine_similarity_loss: 74.64063262939453
  suffix vector: tensor([ 0.0790,  0.0957, -0.1964,  ..., -0.2730,  0.2042,  0.0455],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 8.210373878479004
  l2 dist from vocab: 739.5844116210938
  learned embed: Parameter containing:
tensor([[-14.2929,   0.7573,  -7.5877,  ...,  -5.5886,  -4.7118,  -0.4821]],
       device='cuda:0', requires_grad=True)
EPOCH 500
  cosine_similarity_loss: 49.29716491699219
  suffix vector: tensor([ 0.0782,  0.1035, -0.1907,  ..., -0.2657,  0.2071,  0.0540],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 8.177331924438477
  l2 dist from vocab: 486.2012939453125
  learned embed: Parameter containing:
tensor([[-0.2157, -1.6852, -0.7689,  ..., -0.0749, -0.1392, -0.5313]],
       device='cuda:0', requires_grad=True)
EPOCH 1000
  cosine_similarity_loss: 47.89715576171875
  suffix vector: tensor([ 0.0642,  0.1049, -0.1984,  ..., -0.2484,  0.1941,  0.0569],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 8.01130485534668
  l2 dist from vocab: 472.2928161621094
  learned embed: Parameter containing:
tensor([[ 0.1413, -1.6549, -0.5279,  ..., -0.1140, -0.1403, -0.5228]],
       device='cuda:0', requires_grad=True)
EPOCH 1500
  cosine_similarity_loss: 47.81377029418945
  suffix vector: tensor([ 0.0722,  0.1078, -0.2034,  ..., -0.2308,  0.1789,  0.0689],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 7.864872455596924
  l2 dist from vocab: 471.6070251464844
  learned embed: Parameter containing:
tensor([[ 0.2277, -1.6453, -0.0746,  ..., -0.1180, -0.2003, -0.5546]],
       device='cuda:0', requires_grad=True)
token after optim: serrure
suffix seq:  serrure serrure serrure
