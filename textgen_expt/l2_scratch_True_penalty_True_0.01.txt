t5_encoder is in training mode: False
using loss function l2 with True penalty, alpha = 0.01
init suffix token: Aus
EPOCH 0
  l2_loss: 15.359310150146484
  suffix vector: tensor([-0.0489,  0.2175,  0.1051,  ..., -0.2017,  0.1707, -0.0437],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 8.312365531921387
  l2 dist from vocab: 704.6944580078125
  learned embed: Parameter containing:
tensor([[  2.4964,   4.6616,  21.3218,  ...,   7.4788,   9.5720, -10.3190]],
       device='cuda:0', requires_grad=True)
STOPPING AT EPOCH 463
token after optim: mes
suffix seq:  mes
using loss function l2 with True penalty, alpha = 0.01
init suffix token: Bezug
EPOCH 0
  l2_loss: 16.033004760742188
  suffix vector: tensor([-0.2249, -0.0763, -0.1399,  ..., -0.2588,  0.0389, -0.0736],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 8.712026596069336
  l2 dist from vocab: 732.0978393554688
  learned embed: Parameter containing:
tensor([[  4.2359,  -5.3316,  -8.7689,  ..., -27.2074,   5.2091,  22.3614]],
       device='cuda:0', requires_grad=True)
EPOCH 500
  l2_loss: 8.898822784423828
  suffix vector: tensor([-0.0362,  0.0499,  0.0533,  ..., -0.0557,  0.1364,  0.0715],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 3.3441174030303955
  l2 dist from vocab: 555.4705810546875
  learned embed: Parameter containing:
tensor([[  5.8899,  -1.2391,  -2.9513,  ..., -10.0766,   4.7734,   2.6712]],
       device='cuda:0', requires_grad=True)
EPOCH 1000
  l2_loss: 8.358461380004883
  suffix vector: tensor([-0.0498,  0.0294,  0.0379,  ..., -0.0433,  0.1667,  0.1025],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 2.857494354248047
  l2 dist from vocab: 550.0966796875
  learned embed: Parameter containing:
tensor([[ 6.5865, -1.1122, -2.4079,  ..., -5.7074,  5.2220, -0.2197]],
       device='cuda:0', requires_grad=True)
EPOCH 1500
  l2_loss: 8.132730484008789
  suffix vector: tensor([-0.0754,  0.0390,  0.0076,  ..., -0.0235,  0.1455,  0.1207],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 2.656933069229126
  l2 dist from vocab: 547.5797729492188
  learned embed: Parameter containing:
tensor([[ 5.9542, -0.9484, -2.7386,  ..., -4.0181,  5.2046, -0.8618]],
       device='cuda:0', requires_grad=True)
token after optim: mes
suffix seq:  mes mes
using loss function l2 with True penalty, alpha = 0.01
init suffix token: samples
EPOCH 0
  l2_loss: 14.739014625549316
  suffix vector: tensor([ 0.0244,  0.0512,  0.2016,  ..., -0.1078, -0.0726, -0.0421],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 8.606890678405762
  l2 dist from vocab: 613.21240234375
  learned embed: Parameter containing:
tensor([[-11.2353,  10.9471,  -7.5395,  ...,  13.4517,  -6.0137,   2.9889]],
       device='cuda:0', requires_grad=True)
EPOCH 500
  l2_loss: 8.446004867553711
  suffix vector: tensor([-0.0935,  0.0679,  0.0752,  ..., -0.0307,  0.1026,  0.1384],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 3.1404671669006348
  l2 dist from vocab: 530.5537719726562
  learned embed: Parameter containing:
tensor([[-4.3761, 10.0131, -3.3006,  ..., -1.8105,  6.5639, -6.4367]],
       device='cuda:0', requires_grad=True)
EPOCH 1000
  l2_loss: 7.870174407958984
  suffix vector: tensor([-0.1137,  0.1021,  0.0408,  ..., -0.0138,  0.1352,  0.1234],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 2.583540678024292
  l2 dist from vocab: 528.6633911132812
  learned embed: Parameter containing:
tensor([[-4.8045, 10.9613, -3.5139,  ..., -4.2457,  6.8714, -4.8604]],
       device='cuda:0', requires_grad=True)
EPOCH 1500
  l2_loss: 7.663295745849609
  suffix vector: tensor([-0.0879,  0.0972,  0.0041,  ...,  0.0049,  0.1365,  0.1369],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
  vector l2 distance: 2.388453960418701
  l2 dist from vocab: 527.4841918945312
  learned embed: Parameter containing:
tensor([[-3.4859, 11.1351, -3.1846,  ..., -5.2511,  6.6847, -4.6724]],
       device='cuda:0', requires_grad=True)
token after optim: mes
suffix seq:  mes mes mes
